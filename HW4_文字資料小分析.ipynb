{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/lTSbKRXDS+rBTONcb+wy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophielu05/114-1-/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil"
      ],
      "metadata": {
        "id": "pvg6ykjmnxnd"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import service_account\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "AptzZMDznyIX"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "-OCDa_l2nyN8"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# 從 Colab Secrets 中獲取 API 金鑰\n",
        "api_key = userdata.get('lty')\n",
        "\n",
        "# 使用獲取的金鑰配置 genai\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-pro')"
      ],
      "metadata": {
        "id": "9fJhzbR_nyVM"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/10mSu-t3IZxMocPZyGKaP6ria3MxIlYnsTgFpU11Oh2s/edit?gid=0#gid=0\"\n",
        "WORKSHEET_NAME = \"工作表1\"\n",
        "TIMEZONE = \"Asia/Taipei\""
      ],
      "metadata": {
        "id": "qLvjxDHDnyaz"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]"
      ],
      "metadata": {
        "id": "oPISSK4DoKOj"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_spreadsheet(name):\n",
        "    try:\n",
        "        sh = gc.open(name)  # returns gspread.models.Spreadsheet\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)"
      ],
      "metadata": {
        "id": "lUchh6qWoKRG"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_worksheet(sh, title, header):\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # 若沒有表頭就補上\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws"
      ],
      "metadata": {
        "id": "Dl3J4RagoKTq"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_movie_terms\", TERMS_HEADER)"
      ],
      "metadata": {
        "id": "tyUirl1toVPO"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import pandas as pd\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import gradio as gr\n",
        "from typing import Tuple, List\n",
        "\n",
        "# --- 配置區 ---\n",
        "# *** 請替換成您的 Gemini API Key (留空的話，Canvas 會自動注入) ***\n",
        "GEMINI_API_KEY = \"lty\"\n",
        "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent\"\n",
        "# 爬蟲設定\n",
        "PTT_URL = 'https://www.ptt.cc'\n",
        "STARTING_BOARD_URL = 'https://www.ptt.cc/bbs/movie/index.html'\n",
        "TARGET_PAGES = 10\n",
        "# Google Sheet 設定\n",
        "SHEET_NAME = \"PTT Movie Analysis\"\n",
        "JSON_KEY_FILE = 'service_account.json'\n",
        "# TF-IDF 設定\n",
        "TOP_N_KEYWORDS = 20\n",
        "# Gradio UI 設定\n",
        "TITLE = \"PTT Movie 版數據分析工具\"\n",
        "\n",
        "# --- 輔助函式：API 呼叫與重試機制 ---\n",
        "\n",
        "def call_gemini_api(prompt: str, system_instruction: str) -> str:\n",
        "    \"\"\"串接 Gemini API 並處理指數退避重試\"\"\"\n",
        "    if not GEMINI_API_KEY:\n",
        "        # 如果在 Canvas 環境運行，API Key 將自動注入\n",
        "        api_key_placeholder = \"lty\"\n",
        "    else:\n",
        "        api_key_placeholder = GEMINI_API_KEY\n",
        "\n",
        "    url = f\"{GEMINI_API_URL}?key={api_key_placeholder}\"\n",
        "\n",
        "    # payload 結構\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_instruction}]},\n",
        "    }\n",
        "\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                url,\n",
        "                headers={'Content-Type': 'application/json'},\n",
        "                data=json.dumps(payload)\n",
        "            )\n",
        "            response.raise_for_status()  # 檢查 HTTP 錯誤\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            # 解析文本內容\n",
        "            text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '').strip()\n",
        "\n",
        "            if text:\n",
        "                return text\n",
        "            else:\n",
        "                print(f\"API 響應成功但未包含文本內容: {result}\")\n",
        "                return \"API 返回結果結構異常，請檢查日誌。\"\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"API 呼叫失敗 (嘗試 {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                # 指數退避\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"等待 {wait_time} 秒後重試...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                return f\"API 呼叫失敗，已達到最大重試次數。錯誤: {e}\"\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"API 返回的不是有效的 JSON。\")\n",
        "            return \"API 返回資料解析失敗。\"\n",
        "    return \"API 呼叫最終失敗。\"\n",
        "\n",
        "# --- 步驟 1: PTT 爬蟲 ---\n",
        "\n",
        "def crawl_ptt(start_url: str, pages: int) -> pd.DataFrame:\n",
        "    \"\"\"爬取 PTT Movie 版指定頁數的文章列表\"\"\"\n",
        "    global PTT_URL\n",
        "\n",
        "    print(f\"開始爬取 PTT Movie 版，共 {pages} 頁...\")\n",
        "    all_posts = []\n",
        "    current_url = start_url\n",
        "\n",
        "    for i in range(pages):\n",
        "        print(f\"  > 正在爬取第 {i+1} 頁: {current_url}\")\n",
        "        try:\n",
        "            # 設置 User-Agent 模仿瀏覽器\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "\n",
        "            # 修正爬蟲問題：加入 over18 Cookie 以繞過 PTT 年齡確認頁面\n",
        "            cookies = {'over18': '1'}\n",
        "\n",
        "            response = requests.get(current_url, headers=headers, cookies=cookies)\n",
        "            response.raise_for_status() # 檢查 HTTP 狀態碼\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            posts = soup.find_all('div', class_='r-ent')\n",
        "\n",
        "            for post in posts:\n",
        "                title_tag = post.find('div', class_='title').find('a')\n",
        "\n",
        "                if title_tag:\n",
        "                    title = title_tag.text.strip()\n",
        "                    href = PTT_URL + title_tag['href']\n",
        "                else:\n",
        "                    # 處理被刪除的文章\n",
        "                    title = '[已被刪除]'\n",
        "                    href = None\n",
        "\n",
        "                author = post.find('div', class_='author').text.strip()\n",
        "                date = post.find('div', class_='date').text.strip()\n",
        "\n",
        "                all_posts.append({\n",
        "                    'Title': title,\n",
        "                    'Author': author,\n",
        "                    'Date': date,\n",
        "                    'Href': href,\n",
        "                    'Keywords': '', # 預留給分析結果\n",
        "                })\n",
        "\n",
        "            # 尋找「上頁」連結以繼續爬取\n",
        "            paging = soup.find('div', class_='btn-group btn-group-paging')\n",
        "            if paging:\n",
        "                # PTT 頁面的第二個 'a' 連結是 '上頁' (即更舊的文章頁面)\n",
        "                prev_page_btn = paging.find_all('a')[1]\n",
        "                if 'href' in prev_page_btn.attrs:\n",
        "                    current_url = PTT_URL + prev_page_btn['href']\n",
        "                else:\n",
        "                    print(\"  > 已到達最舊的文章頁面，停止爬蟲。\")\n",
        "                    break\n",
        "            else:\n",
        "                print(\"  > 無法找到分頁按鈕，停止爬蟲。\")\n",
        "                break\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"爬取第 {i+1} 頁失敗: {e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"處理第 {i+1} 頁時發生錯誤: {e}\")\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(all_posts)\n",
        "    print(f\"爬蟲完成。共抓取 {len(df)} 篇文章。\")\n",
        "    return df\n",
        "\n",
        "# --- 步驟 2 & 4: Google Sheets 讀寫 ---\n",
        "\n",
        "def sheets_io_write(df_posts: pd.DataFrame, df_keywords: pd.DataFrame) -> str:\n",
        "    \"\"\"將爬蟲結果和關鍵字分析結果寫入 Google Sheets\"\"\"\n",
        "    try:\n",
        "        # 使用服務帳號認證\n",
        "        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "        # 嘗試從 JSON_KEY_FILE 載入憑證\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_name(JSON_KEY_FILE, scope)\n",
        "        gc = gspread.authorize(creds)\n",
        "\n",
        "        # 開啟或建立試算表\n",
        "        try:\n",
        "            sh = gc.open(SHEET_NAME)\n",
        "            print(f\"成功開啟 Google Sheet: '{SHEET_NAME}'。\")\n",
        "        except gspread.SpreadsheetNotFound:\n",
        "            print(f\"Google Sheet '{SHEET_NAME}' 不存在，正在建立新的試算表...\")\n",
        "            sh = gc.create(SHEET_NAME)\n",
        "            # 建議用戶手動共享： sh.share(None, perm_type='user', role='writer', emailAddress=creds.service_account_email)\n",
        "            # 注意：這裡無法自動共享，使用者必須手動將服務帳號 Email 加入編輯者\n",
        "            print(f\"✅ 已建立新的 Google Sheet。請務必將服務帳號 {creds.service_account_email} 加入編輯者名單。\")\n",
        "\n",
        "        # 工作表標題\n",
        "        worksheet_title_data = \"PTT_Data\"\n",
        "        worksheet_title_analysis = \"Keywords_Analysis\"\n",
        "\n",
        "        # --- 寫入爬蟲結果 (PTT_Data) ---\n",
        "        data_msg = \"PTT_Data 寫入狀態: \"\n",
        "        if not df_posts.empty:\n",
        "            try:\n",
        "                # 嘗試取得現有工作表\n",
        "                worksheet_data = sh.worksheet(worksheet_title_data)\n",
        "                worksheet_data.clear()\n",
        "            except gspread.WorksheetNotFound:\n",
        "                # 找不到則建立新的工作表\n",
        "                # add_worksheet 至少需要 1 列和 1 行\n",
        "                worksheet_data = sh.add_worksheet(title=worksheet_title_data, rows=\"1\", cols=\"1\")\n",
        "\n",
        "            # 使用 dataframe 寫入，包含標頭\n",
        "            sh.values_update(\n",
        "                worksheet_title_data,\n",
        "                params={'valueInputOption': 'USER_ENTERED'},\n",
        "                # 結合標題行和數據行\n",
        "                body={'values': [df_posts.columns.values.tolist()] + df_posts.values.tolist()}\n",
        "            )\n",
        "            data_msg += f\"✅ 爬蟲結果已成功寫入。\"\n",
        "        else:\n",
        "            data_msg += \"❌ 爬蟲結果為空，跳過寫入。\"\n",
        "\n",
        "\n",
        "        # --- 寫入 TF-IDF 分析結果 (Keywords_Analysis) ---\n",
        "        keywords_msg = \"Keywords_Analysis 寫入狀態: \"\n",
        "        if not df_keywords.empty:\n",
        "            try:\n",
        "                worksheet_analysis = sh.worksheet(worksheet_title_analysis)\n",
        "                worksheet_analysis.clear()\n",
        "            except gspread.WorksheetNotFound:\n",
        "                worksheet_analysis = sh.add_worksheet(title=worksheet_title_analysis, rows=\"1\", cols=\"1\")\n",
        "\n",
        "            # 使用 dataframe 寫入，包含標頭\n",
        "            sh.values_update(\n",
        "                worksheet_title_analysis,\n",
        "                params={'valueInputOption': 'USER_ENTERED'},\n",
        "                # 結合標題行和數據行\n",
        "                body={'values': [df_keywords.columns.values.tolist()] + df_keywords.values.tolist()}\n",
        "            )\n",
        "            keywords_msg += \"✅ 關鍵字分析結果已成功寫入。\"\n",
        "        else:\n",
        "            keywords_msg += \"❌ 關鍵字分析結果為空，跳過寫入。\"\n",
        "\n",
        "        return f\"--- Google Sheet 同步狀態 ---\\n{data_msg}\\n{keywords_msg}\"\n",
        "\n",
        "    except gspread.exceptions.APIError as e:\n",
        "        error_details = json.loads(e.response.text).get('error', {}).get('message', '未知 API 錯誤')\n",
        "        print(f\"Google Sheets API 錯誤: {error_details}\")\n",
        "        return f\"❌ Google Sheets API 錯誤！請檢查服務帳號是否有**編輯**權限。錯誤詳情：{error_details}\"\n",
        "    except FileNotFoundError:\n",
        "        print(f\"錯誤：找不到服務帳號金鑰檔案 '{JSON_KEY_FILE}'。\")\n",
        "        return f\"❌ 錯誤：找不到金鑰檔案 '{JSON_KEY_FILE}'。請確保檔案已上傳到程式碼同一目錄。\"\n",
        "    except Exception as e:\n",
        "        print(f\"Google Sheets 操作失敗: {e}\")\n",
        "        return f\"❌ Google Sheets 操作失敗: {e}\"\n",
        "\n",
        "# --- 步驟 3: 詞頻與關鍵字統計 (TF-IDF) ---\n",
        "\n",
        "def analyze_text(df: pd.DataFrame, top_n: int) -> Tuple[pd.DataFrame, str]:\n",
        "    \"\"\"使用 TF-IDF 進行中文分詞和關鍵字提取\"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(), \"錯誤：輸入數據為空，無法進行分析。\"\n",
        "\n",
        "    # 合併所有標題作為文本資料\n",
        "    documents = df['Title'].tolist()\n",
        "\n",
        "    # 執行中文分詞\n",
        "    corpus = []\n",
        "    print(\"開始執行 Jieba 中文分詞...\")\n",
        "    for doc in documents:\n",
        "        # 過濾掉刪除的文章標記\n",
        "        if doc == '[已被刪除]':\n",
        "            continue\n",
        "        # 使用 jieba.cut 進行分詞，並將結果以空格連接\n",
        "        words = jieba.cut(doc)\n",
        "        corpus.append(\" \".join(words))\n",
        "\n",
        "    if not corpus:\n",
        "        return pd.DataFrame(), \"錯誤：所有文章標題皆為空或已刪除，無法進行分析。\"\n",
        "\n",
        "    # 建立 TF-IDF 模型\n",
        "    # min_df=5: 排除出現次數過少的詞彙，提升關鍵字品質\n",
        "    # max_df=0.8: 排除過於常見的詞彙 (如 '的', '是', '了')\n",
        "    vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\n",
        "\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    except ValueError as e:\n",
        "        print(f\"TFIDF fit_transform 失敗: {e}\")\n",
        "        return pd.DataFrame(), \"錯誤：TF-IDF 處理失敗，詞彙量不足或設定太嚴格。\"\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # 計算每個詞彙的平均 TF-IDF 分數 (作為關鍵字熱度)\n",
        "    # 取矩陣的總和，除以文章數，得到平均權重\n",
        "    scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
        "\n",
        "    # 建立關鍵字 DataFrame\n",
        "    df_keywords = pd.DataFrame({'Keyword': feature_names, 'TFIDF_Score': scores})\n",
        "\n",
        "    # 排序並取得前 N 個熱詞\n",
        "    df_keywords = df_keywords.sort_values(by='TFIDF_Score', ascending=False).head(top_n).reset_index(drop=True)\n",
        "\n",
        "    # 將分數格式化為小數點後兩位\n",
        "    df_keywords['TFIDF_Score'] = df_keywords['TFIDF_Score'].round(2)\n",
        "\n",
        "    # 生成熱詞字串用於 LLM 摘要\n",
        "    hot_words_list = df_keywords['Keyword'].tolist()\n",
        "    hot_words_str = \"、\".join(hot_words_list)\n",
        "\n",
        "    print(f\"TF-IDF 分析完成。前 {top_n} 熱詞: {hot_words_str[:100]}...\")\n",
        "\n",
        "    return df_keywords, hot_words_str\n",
        "\n",
        "# --- 步驟 5: Gemini API 摘要 ---\n",
        "\n",
        "def generate_insight(hot_words: str, titles: List[str]) -> str:\n",
        "    \"\"\"使用 Gemini API 根據關鍵字和標題生成洞察摘要和結論\"\"\"\n",
        "    if not hot_words:\n",
        "        return \"無法生成摘要，因為沒有提取到關鍵字。\"\n",
        "\n",
        "    # 將標題列表轉為單一字串，限制長度以避免超過 API 限制\n",
        "    titles_sample = \"\\n\".join(titles[:50]) # 取前 50 篇文章標題作為樣本\n",
        "\n",
        "    system_prompt = (\n",
        "        \"你是一位專業的影視市場分析師，專門分析 PTT 上的電影討論熱度。你的回應必須是繁體中文。\"\n",
        "        \"任務是根據提供的電影版文章標題和 TF-IDF 熱詞，總結出市場洞察。\"\n",
        "        \"請嚴格按照以下格式輸出：\"\n",
        "        \"1. 五句簡潔有力的市場洞察摘要。\"\n",
        "        \"2. 一段 120 字的總結結論。\"\n",
        "    )\n",
        "\n",
        "    user_query = (\n",
        "        \"請根據以下的資訊，生成五句洞察摘要和一段 120 字的結論：\\n\\n\"\n",
        "        f\"【關鍵字熱詞 (依熱度排序)】: {hot_words}\\n\\n\"\n",
        "        f\"【近期文章標題範例 (部分)】:\\n{titles_sample}\\n\\n\"\n",
        "        \"請嚴格按照要求，先是五句洞察摘要，接著是一段 120 字的總結結論。\"\n",
        "    )\n",
        "\n",
        "    print(\"呼叫 Gemini API 進行文本摘要與洞察生成...\")\n",
        "    llm_output = call_gemini_api(user_query, system_prompt)\n",
        "\n",
        "    return llm_output\n",
        "\n",
        "# --- 步驟 6: 完整執行流程 ---\n",
        "\n",
        "def full_pipeline(pages: int, top_n: int) -> Tuple[str, str, str]:\n",
        "    \"\"\"執行整個爬蟲、分析、寫入和 LLM 摘要的自動化流程\"\"\"\n",
        "\n",
        "    # 0. 初始化狀態\n",
        "    df_posts_result = pd.DataFrame()\n",
        "    df_keywords_result = pd.DataFrame()\n",
        "    hot_words_str = \"\"\n",
        "    llm_insight = \"等待執行...\"\n",
        "\n",
        "    try:\n",
        "        result_msg = \"--- 執行狀態 ---\\n\"\n",
        "\n",
        "        # 1. 執行 PTT 爬蟲\n",
        "        df_posts_result = crawl_ptt(STARTING_BOARD_URL, pages)\n",
        "\n",
        "        if df_posts_result.empty:\n",
        "            result_msg += \"❌ 爬蟲失敗或未抓取到任何文章。可能是網路問題或 PTT 頁面結構變動。\\n\"\n",
        "            # 即使爬蟲失敗，也嘗試進行 Sheets 寫入，以便將空數據報告到 Sheets\n",
        "            pass # 流程繼續\n",
        "        else:\n",
        "            result_msg += f\"✅ 爬蟲成功：共 {len(df_posts_result)} 篇文章。\\n\"\n",
        "\n",
        "        # 2. 詞頻與關鍵字統計 (TF-IDF)\n",
        "        if not df_posts_result.empty:\n",
        "            df_keywords_result, hot_words_str = analyze_text(df_posts_result, top_n)\n",
        "\n",
        "            if df_keywords_result.empty:\n",
        "                result_msg += \"❌ TF-IDF 分析失敗。\\n\"\n",
        "            else:\n",
        "                result_msg += f\"✅ TF-IDF 分析成功：已提取前 {top_n} 個熱詞。\\n\"\n",
        "        else:\n",
        "            result_msg += \"ℹ️ 爬蟲結果為空，跳過 TF-IDF 分析。\\n\"\n",
        "\n",
        "        # 3. 寫入 Google Sheet (回寫統計表和爬蟲結果)\n",
        "        sheet_write_status = sheets_io_write(df_posts=df_posts_result, df_keywords=df_keywords_result)\n",
        "        result_msg += sheet_write_status + \"\\n\"\n",
        "\n",
        "        # 4. 串接 Gemini API 生成摘要\n",
        "        titles_for_llm = df_posts_result['Title'].tolist() if not df_posts_result.empty else []\n",
        "        if hot_words_str and titles_for_llm:\n",
        "            llm_insight = generate_insight(hot_words_str, titles_for_llm)\n",
        "            result_msg += \"✅ LLM 洞察摘要已生成。\\n\"\n",
        "        else:\n",
        "            llm_insight = \"無法生成摘要，因為爬蟲或關鍵字提取失敗。\"\n",
        "            result_msg += \"❌ LLM 摘要生成失敗。\\n\"\n",
        "\n",
        "        # 5. 準備 Gradio 輸出\n",
        "\n",
        "        # 格式化關鍵字 DataFrame 為 Markdown 輸出\n",
        "        df_analysis_output = df_keywords_result.to_markdown(index=False) if not df_keywords_result.empty else \"無關鍵字分析結果。\"\n",
        "\n",
        "        # 格式化 LLM 輸出\n",
        "        llm_display = \"--- 市場洞察與結論 ---\\n\" + llm_insight\n",
        "\n",
        "        result_msg += \"\\n--- 流程完成 ---\"\n",
        "        return result_msg, df_analysis_output, llm_display\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"致命錯誤：流程執行中發生未預期的錯誤 -> {e}\"\n",
        "        print(error_msg)\n",
        "        return error_msg, \"發生錯誤\", \"發生錯誤\"\n",
        "\n",
        "# --- Gradio 介面 ---\n",
        "\n",
        "# 提示使用者準備金鑰\n",
        "initial_message = (\n",
        "    \"## ⚙️ 環境準備與除錯\\n\"\n",
        "    \"如果您遇到 Google Sheet 寫入失敗，請務必檢查以下兩點：\\n\"\n",
        "    \"1. **Google Sheets 服務帳號金鑰**：名為 `service_account.json` 的檔案是否已上傳到此腳本的同一目錄。\\n\"\n",
        "    \"2. **Sheet 權限**：請確認您服務帳號的 Email 已被加入您 Google Sheet 的**編輯者**名單中。\\n\"\n",
        "    \"3. **Jieba 詞典**：程式會自動使用 Jieba 進行中文分詞。\\n\"\n",
        ")\n",
        "\n",
        "# 定義 Gradio 介面\n",
        "with gr.Blocks(title=TITLE) as demo:\n",
        "    gr.Markdown(f\"# {TITLE}\")\n",
        "    gr.Markdown(initial_message)\n",
        "\n",
        "    with gr.Row():\n",
        "        pages_input = gr.Slider(\n",
        "            minimum=1,\n",
        "            maximum=20,\n",
        "            step=1,\n",
        "            value=TARGET_PAGES,\n",
        "            label=\"爬蟲頁數 (PTT Movie 版)\",\n",
        "            info=\"選擇從最新頁開始往前抓取的頁數。\"\n",
        "        )\n",
        "        keywords_input = gr.Slider(\n",
        "            minimum=5,\n",
        "            maximum=50,\n",
        "            step=1,\n",
        "            value=TOP_N_KEYWORDS,\n",
        "            label=\"TF-IDF 熱詞數量 (N)\",\n",
        "            info=\"選擇要提取和回寫到 Google Sheet 的熱門關鍵字數量。\"\n",
        "        )\n",
        "\n",
        "    run_button = gr.Button(\"一鍵執行爬蟲、分析與摘要\")\n",
        "\n",
        "    # 狀態輸出\n",
        "    status_output = gr.Textbox(label=\"執行狀態與進度（包含 Google Sheet 寫入狀態）\", lines=10, show_copy_button=True)\n",
        "\n",
        "    # 分析結果輸出\n",
        "    with gr.Row():\n",
        "        keyword_output = gr.Markdown(label=\"TF-IDF 關鍵字統計結果 (Markdown)\", show_copy_button=True)\n",
        "        llm_output = gr.Textbox(label=\"Gemini AI 洞察摘要與結論\", lines=15, show_copy_button=True)\n",
        "\n",
        "    # 綁定按鈕和執行函數\n",
        "    # full_pipeline 輸出 (status_msg, df_markdown, llm_insight)\n",
        "    run_button.click(\n",
        "        full_pipeline,\n",
        "        inputs=[pages_input, keywords_input],\n",
        "        outputs=[status_output, keyword_output, llm_output]\n",
        "    )\n",
        "\n",
        "# 在 Jupyter/Colab/Canvas 環境中，需要呼叫 launch() 來顯示介面\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "QXnyEBchJye5",
        "outputId": "1a866cab-d155-4a64-f827-b5526d4ea962"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cfcdd2d1c06846c269.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cfcdd2d1c06846c269.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}